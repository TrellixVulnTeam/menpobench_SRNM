#!/usr/bin/env python
# encoding: utf-8
"""The Menpo Deformable Model benchmarking suite

<experiment_config> can be either the name of a predefined experiment, or the
path to a .yaml experiment configuration file.

Usage:
  menpobench <experiment_config> [--output <dir>] [--overwrite] [--mat]
  menpobench --list
  menpobench (-h | --help)
  menpobench --version

Options:
  --output -o <dir>  Output directory [default: ./menpobench_result].
  --overwrite        Any existing output dir will be removed.
  --mat              A Matlab .mat file will be saved out for each method.
  --list             List all available predefined experiment components.
  -h --help          Show this screen.
  --version          Show version.
"""
import menpobench
import traceback
from menpobench import (invoke_benchmark, configure_cache_dir,
                        configure_matlab_bin_path)
from docopt import docopt
from menpobench.method.matlab.base import resolve_matlab_bin_path
from menpobench.config import BenchMissingConfigError
from menpobench.utils import centre_str
from menpobench.schema import SchemaError, MissingMetadataError


def invoke_benchmark_with_config_prompts(*args, **kwargs):
    try:
        # TODO: Lazily find Matlab here - this should be fired off by YAML validation
        resolve_matlab_bin_path(verbose=True)
        invoke_benchmark(*args, **kwargs)
    except BenchMissingConfigError as e:
        if e.message == 'cache_dir':
            print('Welcome to menpobench. To start, you will need to choose a directory')
            print('that menpobench can use as a cache.')
            print('This directory will be managed by menpobench to store datasets and')
            print('temporary results. Anticipate it to get quite large (~20GB).')
            cache_dir = raw_input('Please enter cache directory: ')
            configure_cache_dir(cache_dir)
            # now we have a cache dir, re-run
            invoke_benchmark_with_config_prompts(*args, **kwargs)
        elif e.message == 'matlab_bin_path':
            print('To use a Matlab method you must have installed and '
                  'configured Matlab correctly.')
            print('menpobench was unable to automatically find the Matlab executable '
                  '- please')
            print('specify the path to the Matlab executable, '
                  "usually found in the 'bin' directory")
            matlab_bin_dir = raw_input('Please enter Matlab executable path: ')
            configure_matlab_bin_path(matlab_bin_dir)
            # now we have a Matlab bin path, re-run
            invoke_benchmark_with_config_prompts(*args, **kwargs)
        else:
            print('Unexpected missing config value: {}'.format(e))
            exit(1)
    except SchemaError as e:
        print('')
        print(centre_str('SCHEMA ERROR'))
        print('')
        print(e)
        print('\nCorrect this issue and try running menpobench again.')
        exit(1)
    except MissingMetadataError as e:
        print('')
        print(centre_str('MISSING METADATA ERROR'))
        print('')
        print(e)
        print('\nCorrect this issue and try running menpobench again.')
        exit(1)
    except Exception:
        print('\n')
        print(centre_str('ERROR', c='!'))
        print(centre_str('-- menpobench has encountered an unrecoverable error --'))
        print(' ')
        print(traceback.format_exc())
        exit(1)


def list_all_predefined():
    from menpobench.experiment import list_predefined_experiments
    from menpobench.dataset import list_predefined_datasets
    from menpobench.method import (list_predefined_methods,
                                   list_predefined_untrainable_methods)
    from menpobench.lmprocess import list_predefined_lm_processes
    import yaml
    print(yaml.dump({'datasets': list_predefined_datasets(),
                     'methods': list_predefined_methods(),
                     'untrainable_methods': list_predefined_untrainable_methods(),
                     'experiments': list_predefined_experiments(),
                     'landmark_processes': list_predefined_lm_processes()
                     }, default_flow_style=False))


if __name__ == '__main__':
    a = docopt(__doc__,
               version='menpobench v{}'.format(menpobench.__version__))
    if a['--list']:
        list_all_predefined()
    else:
        invoke_benchmark_with_config_prompts(a['<experiment_config>'],
                                             a['--output'],
                                             overwrite=a['--overwrite'],
                                             matlab=a['--mat'])
